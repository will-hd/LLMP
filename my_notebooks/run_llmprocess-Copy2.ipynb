{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1dbcdd28-f8ca-4995-9839-1f066b9767e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "sys.path.append('..')\n",
    "import pickle\n",
    "from typing import NamedTuple, Literal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca990556-8a3d-4554-881e-8c8b0b3d7e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMPRegressionDescription(NamedTuple):\n",
    "    x_context: torch.Tensor\n",
    "    y_context: torch.Tensor\n",
    "    x_target: torch.Tensor\n",
    "    y_target: torch.Tensor\n",
    "    knowledge: list[str] | tuple[str] | None\n",
    "    num_total_points: int\n",
    "    num_context_points: int\n",
    "\n",
    "class TempData:\n",
    "    def __init__(self, data: pd.DataFrame, max_num_context: int, device, random_state=42):\n",
    "        self.data = data\n",
    "        self.max_num_context = max_num_context\n",
    "        x_values = data.iloc[0][3:].values.astype('float32') # (288,)\n",
    "        assert x_values.shape[0] == 288\n",
    "        self.x_values = torch.linspace(-2, 2, len(x_values), device=device).unsqueeze(0)\n",
    "        \n",
    "        y_values = data.iloc[1:, 3:].values\n",
    "        y_desc = data.iloc[1:, 2].values\n",
    "        y_values_train, y_values_temp, y_desc_train, y_desc_temp = train_test_split(\n",
    "            y_values, y_desc, test_size=0.3, random_state=random_state\n",
    "        )\n",
    "        y_values_val, y_values_test, y_desc_val, y_desc_test = train_test_split(\n",
    "            y_values_temp, y_desc_temp, test_size=0.5, random_state=random_state\n",
    "        )\n",
    "\n",
    "        self.y_values_train = torch.tensor(y_values_train).float().to(device)\n",
    "        self.y_values_val = torch.tensor(y_values_val).float().to(device)\n",
    "        self.y_values_test = torch.tensor(y_values_test).float().to(device)\n",
    "        self.y_desc_train = y_desc_train\n",
    "        self.y_desc_val = y_desc_val\n",
    "        self.y_desc_test = y_desc_test\n",
    "        \n",
    "        # # self.x_values = torch.from_numpy(x_values).unsqueeze(0).to(device)  # Shape: [1, num_points]\n",
    "        \n",
    "        # self.y_values_train = torch.tensor(data.iloc[1:508, 3:].values).float().to(device)  # Shape: [num_samples, num_points]\n",
    "        # self.y_values_test = torch.tensor(data.iloc[509:618, 3:].values).float().to(device)  # Shape: [num_samples, num_points]\n",
    "        # self.y_values_val = torch.tensor(data.iloc[619:, 3:].values).float().to(device)  # Shape: [num_samples, num_points]\n",
    "        # self.y_desc_train = data.iloc[1:508, 2].values\n",
    "        # self.y_desc_test = data.iloc[509:618, 2].values\n",
    "        # self.y_desc_val = data.iloc[619:, 2].values\n",
    "\n",
    "    def generate_batch(self, \n",
    "                       batch_size: int,\n",
    "                       split: Literal['train', 'val', 'test'],\n",
    "                       device: torch.device = torch.device('cpu'),\n",
    "                       return_knowledge: bool = False,\n",
    "                       num_context: None | int = None\n",
    "                       ) -> LLMPRegressionDescription:\n",
    "        num_total_points = self.x_values.size(-1)\n",
    "        if num_context is None:\n",
    "            num_context = np.random.randint(low=1, high=self.max_num_context)\n",
    "        else:\n",
    "            assert isinstance(num_context, int) \n",
    "        num_target = num_total_points  # Using all points as target\n",
    "\n",
    "        if split == 'train':\n",
    "            selected_indices = np.random.choice(self.y_values_train.size(0), batch_size, replace=False)\n",
    "            selected_y_values = self.y_values_train[selected_indices]  # Shape: [batch_size, num_points]\n",
    "\n",
    "            knowledge = self.y_desc_train[selected_indices]\n",
    "        elif split == 'val':\n",
    "            selected_indices = np.random.choice(self.y_values_val.size(0), batch_size, replace=False)\n",
    "            selected_y_values = self.y_values_val[selected_indices]  # Shape: [batch_size, num_points]\n",
    "\n",
    "            knowledge = self.y_desc_val[selected_indices]\n",
    "\n",
    "        elif split == 'test':\n",
    "            selected_indices = np.random.choice(self.y_values_test.size(0), batch_size, replace=False)\n",
    "            selected_y_values = self.y_values_test[selected_indices]  # Shape: [batch_size, num_points]\n",
    "\n",
    "            knowledge = self.y_desc_test[selected_indices]\n",
    "        else:\n",
    "            raise ValueError(\"split must be one of ['train', 'val', 'test']\")\n",
    "        # Split into context and target sets\n",
    "        context_indices = np.random.choice(num_total_points // 2, num_context, replace=False)\n",
    "\n",
    "        x_context = self.x_values[:, context_indices].repeat(batch_size, 1)  # Shape: [batch_size, num_context]\n",
    "        y_context = selected_y_values[:, context_indices]  # Shape: [batch_size, num_context]\n",
    "\n",
    "        x_target = self.x_values[::4].repeat(batch_size, 1)  # Shape: [batch_size, num_target]\n",
    "        y_target = selected_y_values[::4]  # Shape: [batch_size, num_target]\n",
    "        print(x_target.shape)\n",
    "        if return_knowledge:\n",
    "            \n",
    "            return LLMPRegressionDescription(\n",
    "                x_context=x_context.unsqueeze(-1).to(device),  # Shape: [batch_size, num_context, x_size]\n",
    "                y_context=y_context.unsqueeze(-1).to(device),  # Shape: [batch_size, num_context, y_size]\n",
    "                x_target=x_target.unsqueeze(-1).to(device),    # Shape: [batch_size, num_target, x_size]\n",
    "                y_target=y_target.unsqueeze(-1).to(device),    # Shape: [batch_size, num_target, y_size]\n",
    "                knowledge=list(knowledge), # Shape/type: TODO\n",
    "                num_total_points=num_total_points,\n",
    "                num_context_points=num_context\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            \n",
    "            return LLMPRegressionDescription(\n",
    "                x_context=x_context.unsqueeze(-1).to(device),  # Shape: [batch_size, num_context, x_size]\n",
    "                y_context=y_context.unsqueeze(-1).to(device),  # Shape: [batch_size, num_context, y_size]\n",
    "                x_target=x_target.unsqueeze(-1).to(device),    # Shape: [batch_size, num_target, x_size]\n",
    "                y_target=y_target.unsqueeze(-1).to(device),    # Shape: [batch_size, num_target, y_size]\n",
    "                knowledge=None,\n",
    "                num_total_points=num_total_points,\n",
    "                num_context_points=num_context\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4806aebc-2c68-4373-85b2-5876806cec15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 288])\n",
      "X Training Data: [-1.5400697  -0.77351916 -0.71777004 -1.5121951  -1.4425087  -0.20209058\n",
      " -1.0383276  -1.7351916  -0.6480836  -0.8989547 ]\n",
      "Y Training Data: [ 9.8 10.7 10.7 10.1 10.3 11.8 10.7 10.5 10.8 10.7]\n",
      "X Test Data: (288,)\n",
      "Y Test Data: [ 9.5  9.6  9.7  9.6  9.9 10.2 10.2 10.3 10.4 10.4 10.4 10.5 10.5 10.5\n",
      " 10.5 10.5 10.4 10.5 10.5 10.5 10.5 10.4 10.4 10.5 10.3 10.2 10.4 10.2\n",
      "  9.9  9.9  9.9  9.9  9.9  9.8 10.  10.1 10.1  9.9  9.8 10.  10.3 10.\n",
      "  9.7  9.7  9.5  9.5  9.7  9.7  9.9 10.  10.   9.9  9.8  9.9  9.8  9.8\n",
      "  9.9 10.  10.2 10.3 10.3 10.2 10.1 10.3 10.4 10.6 10.7 10.7 10.8 10.7\n",
      " 10.7 10.7 10.7 10.7 10.7 10.6 10.7 10.7 10.6 10.7 10.6 10.6 10.6 10.6\n",
      " 10.6 10.6 10.7 10.7 10.7 10.7 10.8 10.7 10.7 10.7 10.7 10.7 10.7 10.8\n",
      " 10.8 10.8 10.8 10.8 10.9 10.8 10.5 10.3 10.3 10.3 10.2 10.2 10.1 10.2\n",
      " 10.2 10.2 10.3 10.3 10.4 10.5 10.6 10.6 10.7 10.7 10.8 11.  11.  11.3\n",
      " 11.2 11.4 11.6 11.8 12.2 12.1 12.2 12.3 12.3 12.5 12.7 12.6 12.6 12.5\n",
      " 12.5 12.6 12.5 12.6 12.7 12.8 12.7 12.8 12.8 12.6 12.6 12.5 12.5 12.6\n",
      " 12.6 12.6 12.6 12.7 12.8 12.8 12.8 12.9 13.  12.9 12.9 13.  13.1 13.\n",
      " 13.  13.  13.1 13.1 13.2 13.1 13.1 12.9 12.8 12.8 12.8 12.9 12.9 13.\n",
      " 13.  13.  13.1 13.1 13.2 13.2 13.2 13.2 13.1 13.2 13.2 13.2 13.3 13.4\n",
      " 13.4 13.6 13.7 13.7 13.7 13.8 13.8 13.6 13.6 13.7 13.9 13.8 14.  14.\n",
      " 14.1 14.2 14.3 14.2 14.2 14.1 14.2 14.2 14.2 14.3 14.4 14.5 14.4 14.4\n",
      " 14.3 14.3 14.2 14.2 14.2 14.1 14.  13.9 13.8 13.7 13.6 13.6 13.5 13.4\n",
      " 13.3 13.2 13.1 12.9 12.9 12.8 12.7 12.7 12.6 12.6 12.5 12.4 12.3 12.3\n",
      " 12.2 12.2 12.1 12.  12.  11.9 11.9 11.9 11.9 11.8 11.8 11.7 11.6 11.5\n",
      " 11.5 11.5 11.5 11.5 11.4 11.4 11.4 11.3 11.3 11.2 11.2 11.1 11.2 11.2\n",
      " 11.2 11.2 11.1 11.1 11.1 11.1 11.1 11.1]\n",
      "Knowledge: The night will start off very chilly at around -6 degrees, gradually warming to just below freezing in the early morning hours. The rest of the day will continue to be cold, with temperatures hovering around -3 to -4 degrees in the afternoon, and then dropping further to -7 degrees in the late evening.\n"
     ]
    }
   ],
   "source": [
    "def process_and_save_data(file_path, pkl_output_path, max_num_context=20, device='cpu', random_state=42, return_knowledge=False):\n",
    "\n",
    "    data = pd.read_csv(file_path)\n",
    "    \n",
    "    temp_data = TempData(data=data, max_num_context=max_num_context, device=device, random_state=random_state)\n",
    "    \n",
    "    llmp_description = temp_data.generate_batch(batch_size=1, split='train', device=device, return_knowledge=return_knowledge, num_context=10)\n",
    "\n",
    "    data_to_save = {\n",
    "        'x_train': llmp_description.x_context.cpu().numpy().flatten(),\n",
    "        'y_train': llmp_description.y_context.cpu().numpy().flatten(),\n",
    "        'x_test': llmp_description.x_target.cpu().numpy().flatten(),\n",
    "        'y_test': llmp_description.y_target.cpu().numpy().flatten(),\n",
    "        'x_true': llmp_description.x_target.cpu().numpy().flatten(),\n",
    "        'y_true': llmp_description.y_target.cpu().numpy().flatten(),\n",
    "    }\n",
    "    \n",
    "    with open(pkl_output_path, 'wb') as handle:\n",
    "        pickle.dump(data_to_save, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    knowledge = None\n",
    "    if return_knowledge:\n",
    "        knowledge = data.iloc[0]['description']\n",
    "    \n",
    "    return {\n",
    "        'data': data_to_save,\n",
    "        'knowledge': knowledge\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "result = process_and_save_data('../data/kasia/data_with_desc.csv', 'random_sample_output_data.pkl', return_knowledge=True)\n",
    "print(\"X Training Data:\", result['data']['x_train'])\n",
    "print(\"Y Training Data:\", result['data']['y_train'])\n",
    "print(\"X Test Data:\", result['data']['x_test'].shape)\n",
    "print(\"Y Test Data:\", result['data']['y_test'])\n",
    "print(\"Knowledge:\", result['knowledge'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "304df46d-b4eb-4fab-8fb4-563ea8470a48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# with open(\"../data/functions/linear_25_seed_6.pkl\", \\'rb\\') as f:\\n#     data = pickle.load(f)\\ndata = {\\n    \\'x_train\\': np.array([53., 115., 188.]),  # Select first 3 elements\\n    \\'y_train\\': np.array([-0.44, 0.17, 0.85]),  # Select first 3 elements\\n    \\'x_test\\': np.array([0., 5., 10., 15., 20., 25., 30., 35., 40., 45., 50.,\\n                        55., 60., 65., 70., 75., 80., 85., 90., 95., 100., 105.,\\n                        110., 115., 120., 125., 130., 135., 140., 145., 150., 155., 160.,\\n                        165., 170., 175., 180., 185., 190., 195.]),\\n    \\'y_test\\': [-1.0, -0.95, -0.9, -0.85, -0.8, -0.75, -0.7, -0.65, -0.6, -0.55, -0.5, -0.45, -0.4, -0.35, -0.3, -0.25, -0.2, -0.15, -0.1, -0.05, 0.01, 0.06, 0.11, 0.16, 0.21, 0.26, 0.31, 0.36, 0.41, 0.46, 0.51, 0.56, 0.61, 0.66, 0.71, 0.76, 0.81, 0.86, 0.91, 0.96],\\n    \\'x_true\\': np.array([0., 1., 2., 3., 4.]),\\n    \\'y_true\\': np.array([-1.        , -0.98994975, -0.9798995 , -0.96984925, -0.95979899])\\n}\\n\\nwith open(\\'test_data.pkl\\', \\'wb\\') as handle:\\n    pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# with open(\"../data/functions/linear_25_seed_6.pkl\", 'rb') as f:\n",
    "#     data = pickle.load(f)\n",
    "data = {\n",
    "    'x_train': np.array([53., 115., 188.]),  # Select first 3 elements\n",
    "    'y_train': np.array([-0.44, 0.17, 0.85]),  # Select first 3 elements\n",
    "    'x_test': np.array([0., 5., 10., 15., 20., 25., 30., 35., 40., 45., 50.,\n",
    "                        55., 60., 65., 70., 75., 80., 85., 90., 95., 100., 105.,\n",
    "                        110., 115., 120., 125., 130., 135., 140., 145., 150., 155., 160.,\n",
    "                        165., 170., 175., 180., 185., 190., 195.]),\n",
    "    'y_test': [-1.0, -0.95, -0.9, -0.85, -0.8, -0.75, -0.7, -0.65, -0.6, -0.55, -0.5, -0.45, -0.4, -0.35, -0.3, -0.25, -0.2, -0.15, -0.1, -0.05, 0.01, 0.06, 0.11, 0.16, 0.21, 0.26, 0.31, 0.36, 0.41, 0.46, 0.51, 0.56, 0.61, 0.66, 0.71, 0.76, 0.81, 0.86, 0.91, 0.96],\n",
    "    'x_true': np.array([0., 1., 2., 3., 4.]),\n",
    "    'y_true': np.array([-1.        , -0.98994975, -0.9798995 , -0.96984925, -0.95979899])\n",
    "}\n",
    "\n",
    "with open('test_data.pkl', 'wb') as handle:\n",
    "    pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "0bfcd46f-ebf6-4a94-bd5b-fdcda8a5095a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.plot import plot_samples, plot_images, plot_heatmap\n",
    "from src.hf_api import get_model_and_tokenizer\n",
    "from src.parse_args import parse_command_line\n",
    "from src.compute_nll import compute_nll\n",
    "from src.sample import sample\n",
    "from src.prepare_data import prepare_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f5ab42ad-681a-49fa-9827-643a79e60ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "\n",
    "args = SimpleNamespace(\n",
    "    cfg=None,  # Assuming `ActionConfigFile` is a custom action, replace with the appropriate default if needed.\n",
    "    mode='sample_logpy',\n",
    "    experiment_name='test',\n",
    "    # data_path='../data/functions/linear_25_seed_6.pkl',\n",
    "    data_path='random_sample_output_data.pkl',\n",
    "    llm_path=None,\n",
    "    llm_type=\"llama-3-8B\", # \"llama-3-70B\"\n",
    "    prompt_ordering='distance',\n",
    "    output_dir='./output',\n",
    "    plot_dir='./plots',\n",
    "    seed=1,\n",
    "    num_decimal_places_x=0,\n",
    "    num_decimal_places_y=2,\n",
    "    batch_size=5,\n",
    "    autoregressive=True,\n",
    "    prefix=result['knowledge'],\n",
    "    x_prefix='',\n",
    "    y_prefix=', ',\n",
    "    break_str='\\n',\n",
    "    sort_x_test=False,\n",
    "    forecast=True,\n",
    "    print_prompts=False,\n",
    "    print_logprobs=False,\n",
    "    num_samples=50,\n",
    "    temperature=1.0,\n",
    "    top_p=0.9,\n",
    "    max_generated_length=7,\n",
    "    y_min=None,\n",
    "    y_max=None,\n",
    "    plot_trajectories=5,\n",
    "    specify_xy=False,\n",
    "    xs=None,  # Assuming default is None when no arguments are provided\n",
    "    ys=None,  # Assuming default is None when no arguments are provided\n",
    "    xs_start=None,\n",
    "    xs_end=None,\n",
    "    num_xs=None,\n",
    "    ys_start=None,\n",
    "    ys_end=None,\n",
    "    num_ys=None,\n",
    "    mask_unused_tokens=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "cc9daf7d-9f3c-4c15-94e7-a283d449406d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_HOME'] = '/workspace/will/LLMP/hf_cache/' \n",
    "os.environ['HF_HUB_CACHE'] = '/workspace/will/LLMP/hf_cache/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "294dbb5b-f511-4916-a06a-5755ed5ef51c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [01:16<00:00, 19.12s/it]\n"
     ]
    }
   ],
   "source": [
    "# mistral_token = \"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\"\n",
    "# llama_token = \"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\"\n",
    "# llama_chat_token = \"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\"\n",
    "llama3_token = \"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\"\n",
    "# llama2_70b = \"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\"\n",
    "# llama3_70b = \"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\"\n",
    "# get the llm and asociated tokenizer\n",
    "model, tokenizer = get_model_and_tokenizer(args.llm_path, args.llm_type, llama3_token)\n",
    "# Load model directly\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", token=llama_chat_token)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", token=llama_chat_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16ad9952-c8dd-4de1-a3b5-6ee0172f1e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device 1 is not available, available devices are [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict({'': 0})\n"
     ]
    }
   ],
   "source": [
    "from accelerate import infer_auto_device_map\n",
    "\n",
    "device_map = infer_auto_device_map(model, max_memory={0: \"80GiB\", 1: \"80GiB\",  \"cpu\": \"250GiB\"})\n",
    "print(device_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "52b244cb-5db5-46ef-9746-df9a782b452e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 91it [04:55,  3.25s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[109], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrun_llm_process\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m run_llm_process\n\u001b[0;32m----> 2\u001b[0m \u001b[43mrun_llm_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/will/LLMP/my_notebooks/../src/run_llm_process.py:22\u001b[0m, in \u001b[0;36mrun_llm_process\u001b[0;34m(args, model, tokenizer)\u001b[0m\n\u001b[1;32m     19\u001b[0m results \u001b[38;5;241m=\u001b[39m prepare_data(args)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m args\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m---> 22\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresults\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogpy\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m args\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m     25\u001b[0m     results \u001b[38;5;241m=\u001b[39m compute_nll(args, tokenizer, model, results)\n",
      "File \u001b[0;32m/workspace/will/LLMP/my_notebooks/../src/sample.py:46\u001b[0m, in \u001b[0;36msample\u001b[0;34m(args, tokenizer, model, results)\u001b[0m\n\u001b[1;32m     28\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m construct_prompts(\n\u001b[1;32m     29\u001b[0m         x_train\u001b[38;5;241m=\u001b[39mx_train_current[sample_index],\n\u001b[1;32m     30\u001b[0m         y_train\u001b[38;5;241m=\u001b[39my_train_current[sample_index],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     42\u001b[0m         x_ordering\u001b[38;5;241m=\u001b[39mresults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx_ordering\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx_ordering\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     43\u001b[0m     )\n\u001b[1;32m     44\u001b[0m     batch_prompts\u001b[38;5;241m.\u001b[39mappend(prompt[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m---> 46\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mhf_generate_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_prompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_generated_length\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m(\u001b[38;5;28mlen\u001b[39m(res) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_indices))\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, sample_index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(batch_indices):\n",
      "File \u001b[0;32m/workspace/will/LLMP/.llmp-venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/will/LLMP/my_notebooks/../src/hf_api.py:174\u001b[0m, in \u001b[0;36mhf_generate_batch\u001b[0;34m(model, tokenizer, prompts, temp, top_p, max_new_tokens)\u001b[0m\n\u001b[1;32m    171\u001b[0m batch \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mcuda() \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m    172\u001b[0m num_input_ids \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m--> 174\u001b[0m generate_ids \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrenormalize_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m gen_strs \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(\n\u001b[1;32m    185\u001b[0m     generate_ids[:, num_input_ids:],\n\u001b[1;32m    186\u001b[0m     skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    187\u001b[0m     clean_up_tokenization_spaces\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    188\u001b[0m )\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m gen_strs\n",
      "File \u001b[0;32m/workspace/will/LLMP/.llmp-venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/will/LLMP/.llmp-venv/lib/python3.12/site-packages/transformers/generation/utils.py:2024\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2016\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2017\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2018\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2019\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2020\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2021\u001b[0m     )\n\u001b[1;32m   2023\u001b[0m     \u001b[38;5;66;03m# 13. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2024\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2025\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2026\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2027\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2028\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2029\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2030\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2031\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2032\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2033\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2035\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2036\u001b[0m     \u001b[38;5;66;03m# 11. prepare logits warper\u001b[39;00m\n\u001b[1;32m   2037\u001b[0m     prepared_logits_warper \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2038\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_logits_warper(generation_config, device\u001b[38;5;241m=\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   2039\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m generation_config\u001b[38;5;241m.\u001b[39mdo_sample\n\u001b[1;32m   2040\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2041\u001b[0m     )\n",
      "File \u001b[0;32m/workspace/will/LLMP/.llmp-venv/lib/python3.12/site-packages/transformers/generation/utils.py:2982\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, logits_warper, **model_kwargs)\u001b[0m\n\u001b[1;32m   2979\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[1;32m   2981\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2982\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   2984\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2985\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/will/LLMP/.llmp-venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/will/LLMP/.llmp-venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/will/LLMP/.llmp-venv/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:1189\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1186\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1189\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1190\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1194\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1196\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1197\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1200\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1202\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/workspace/will/LLMP/.llmp-venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/will/LLMP/.llmp-venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/will/LLMP/.llmp-venv/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:1001\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    989\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    990\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    991\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    998\u001b[0m         position_embeddings,\n\u001b[1;32m    999\u001b[0m     )\n\u001b[1;32m   1000\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1001\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1002\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1003\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1004\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1005\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1006\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1007\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1008\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1009\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1012\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/workspace/will/LLMP/.llmp-venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/will/LLMP/.llmp-venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1555\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1552\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1553\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m-> 1555\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1556\u001b[0m     forward_call \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_get_tracing_state() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward)\n\u001b[1;32m   1557\u001b[0m     \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m     \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from src.run_llm_process import run_llm_process\n",
    "run_llm_process(args=args, model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "81863d51-e883-4a43-a371-a653bee7d9b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspace/will/LLMP/my_notebooks'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "cwd = os.getcwd()\n",
    "cwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a814547b-7c7c-4420-9554-09786fda4121",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/functions/beat_05_seed_9.pkl','rb') as f:\n",
    "     contents = pickle.load(f)\n",
    "# '../data/weather/weather_llm_proc_10.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf31d958-982d-4e44-979f-f514ea6ebc1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x_train': array([155.,  58.,  75., 175., 127.]), 'y_train': array([ 0.71,  0.02,  0.6 , -0.41,  0.04]), 'x_test': array([  0.,   5.,  10.,  15.,  20.,  25.,  30.,  35.,  40.,  45.,  50.,\n",
      "        55.,  60.,  65.,  70.,  75.,  80.,  85.,  90.,  95., 100., 105.,\n",
      "       110., 115., 120., 125., 130., 135., 140., 145., 150., 155., 160.,\n",
      "       165., 170., 175., 180., 185., 190., 195.]), 'y_test': [0.0, 0.33, 0.0, -0.81, -0.0, 0.86, 0.0, -0.45, -0.0, -0.2, 0.0, 0.74, 0.0, -0.89, -0.0, 0.56, 0.0, 0.07, 0.0, -0.66, 0.0, 0.9, -0.0, -0.66, -0.0, 0.07, -0.0, 0.56, 0.0, -0.89, 0.0, 0.74, 0.0, -0.2, 0.0, -0.45, -0.0, 0.86, 0.0, -0.81], 'x_true': array([  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,\n",
      "        11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.,  21.,\n",
      "        22.,  23.,  24.,  25.,  26.,  27.,  28.,  29.,  30.,  31.,  32.,\n",
      "        33.,  34.,  35.,  36.,  37.,  38.,  39.,  40.,  41.,  42.,  43.,\n",
      "        44.,  45.,  46.,  47.,  48.,  49.,  50.,  51.,  52.,  53.,  54.,\n",
      "        55.,  56.,  57.,  58.,  59.,  60.,  61.,  62.,  63.,  64.,  65.,\n",
      "        66.,  67.,  68.,  69.,  70.,  71.,  72.,  73.,  74.,  75.,  76.,\n",
      "        77.,  78.,  79.,  80.,  81.,  82.,  83.,  84.,  85.,  86.,  87.,\n",
      "        88.,  89.,  90.,  91.,  92.,  93.,  94.,  95.,  96.,  97.,  98.,\n",
      "        99., 100., 101., 102., 103., 104., 105., 106., 107., 108., 109.,\n",
      "       110., 111., 112., 113., 114., 115., 116., 117., 118., 119., 120.,\n",
      "       121., 122., 123., 124., 125., 126., 127., 128., 129., 130., 131.,\n",
      "       132., 133., 134., 135., 136., 137., 138., 139., 140., 141., 142.,\n",
      "       143., 144., 145., 146., 147., 148., 149., 150., 151., 152., 153.,\n",
      "       154., 155., 156., 157., 158., 159., 160., 161., 162., 163., 164.,\n",
      "       165., 166., 167., 168., 169., 170., 171., 172., 173., 174., 175.,\n",
      "       176., 177., 178., 179., 180., 181., 182., 183., 184., 185., 186.,\n",
      "       187., 188., 189., 190., 191., 192., 193., 194., 195., 196., 197.,\n",
      "       198., 199.]), 'y_true': array([ 0.00000000e+00,  6.72570842e-02,  1.64271721e-17, -2.00268841e-01,\n",
      "       -6.49747766e-17,  3.28806922e-01,  1.43465670e-16, -4.50000000e-01,\n",
      "       -2.48352518e-16,  5.61140822e-01,  3.74837115e-16, -6.59746685e-01,\n",
      "       -5.17032408e-16,  7.43614897e-01,  6.68162400e-16, -8.10871981e-01,\n",
      "       -8.20793921e-16,  8.60015525e-01,  9.67093301e-16, -8.89947744e-01,\n",
      "       -1.09910019e-15,  9.00000000e-01,  4.39751183e-15, -8.89947744e-01,\n",
      "       -1.28945773e-15,  8.60015525e-01, -1.64262499e-15, -8.10871981e-01,\n",
      "       -1.33632480e-15,  7.43614897e-01,  3.79244208e-15, -6.59746685e-01,\n",
      "       -1.19947877e-15,  5.61140822e-01,  2.85668159e-15, -4.50000000e-01,\n",
      "       -8.60794018e-16,  3.28806922e-01,  1.55972304e-15, -2.00268841e-01,\n",
      "       -3.28543442e-16,  6.72570842e-02, -1.08119150e-31,  6.72570842e-02,\n",
      "        1.31450588e-15, -2.00268841e-01, -1.68967260e-15,  3.28806922e-01,\n",
      "        1.14772536e-15, -4.50000000e-01,  2.48980150e-16,  5.61140822e-01,\n",
      "       -2.40047318e-15, -6.59746685e-01,  5.17293735e-15,  7.43614897e-01,\n",
      "        2.67264960e-15, -8.10871981e-01,  1.19066976e-14,  8.60015525e-01,\n",
      "        9.45819585e-15, -8.89947744e-01, -6.59571222e-15,  9.00000000e-01,\n",
      "        3.51712062e-15, -8.89947744e-01, -4.28733014e-16,  8.60015525e-01,\n",
      "        9.44120438e-15, -8.10871981e-01, -6.10987827e-15,  7.43614897e-01,\n",
      "        3.10219445e-15, -6.59746685e-01, -5.98981559e-16,  5.61140822e-01,\n",
      "        5.96171570e-15, -4.50000000e-01, -8.80164483e-15,  3.28806922e-01,\n",
      "        1.29949553e-15, -2.00268841e-01, -2.10317620e-15,  6.72570842e-02,\n",
      "       -4.32476601e-31,  6.72570842e-02,  1.18292245e-15, -2.00268841e-01,\n",
      "       -5.19929575e-15,  3.28806922e-01,  7.64666820e-16, -4.50000000e-01,\n",
      "       -6.45842074e-15,  5.61140822e-01, -3.00097039e-15, -6.59746685e-01,\n",
      "       -4.13625927e-15,  7.43614897e-01,  1.29843356e-14, -8.10871981e-01,\n",
      "        8.22868215e-16,  8.60015525e-01,  8.59747113e-15, -8.89947744e-01,\n",
      "        7.03868544e-15,  9.00000000e-01,  2.63672942e-15, -8.89947744e-01,\n",
      "        1.29010947e-14,  8.60015525e-01, -3.28628712e-15, -8.10871981e-01,\n",
      "       -5.34529920e-15,  7.43614897e-01,  3.24102796e-14, -6.59746685e-01,\n",
      "        1.74000204e-14,  5.61140822e-01,  5.46438303e-15, -4.50000000e-01,\n",
      "       -8.41858630e-15,  3.28806922e-01,  8.57896935e-15, -2.00268841e-01,\n",
      "       -1.97159276e-15,  6.72570842e-02, -9.73072352e-31,  6.72570842e-02,\n",
      "        1.05133901e-15, -2.00268841e-01, -4.93906824e-15,  3.28806922e-01,\n",
      "        3.81608283e-16, -4.50000000e-01, -5.96108807e-15,  5.61140822e-01,\n",
      "        1.37970371e-14, -6.59746685e-01, -3.44601163e-15,  7.43614897e-01,\n",
      "        1.22197565e-14, -8.10871981e-01,  1.64469928e-15,  8.60015525e-01,\n",
      "        7.73674641e-15, -8.89947744e-01, -1.75889363e-14,  9.00000000e-01,\n",
      "        1.75633821e-15, -8.89947744e-01,  1.37618195e-14,  8.60015525e-01,\n",
      "        1.97032027e-14, -8.10871981e-01, -4.58072014e-15,  7.43614897e-01,\n",
      "        3.17200319e-14, -6.59746685e-01,  1.80005176e-14,  5.61140822e-01,\n",
      "        4.96705036e-15, -4.50000000e-01, -8.03552776e-15,  3.28806922e-01,\n",
      "        8.31874183e-15, -2.00268841e-01,  1.97242305e-15,  6.72570842e-02,\n",
      "       -1.72990640e-30,  6.72570842e-02,  9.19755579e-16, -2.00268841e-01,\n",
      "       -4.67884072e-15,  3.28806922e-01,  1.10970955e-14, -4.50000000e-01,\n",
      "       -1.98732225e-14,  5.61140822e-01, -4.20196481e-15, -6.59746685e-01,\n",
      "       -2.75576400e-15,  7.43614897e-01,  1.14551775e-14, -8.10871981e-01,\n",
      "       -2.13447905e-14,  8.60015525e-01,  3.18142277e-14, -8.89947744e-01,\n",
      "        8.79946785e-15,  9.00000000e-01,  8.75947006e-16, -8.89947744e-01,\n",
      "       -1.03156619e-14,  8.60015525e-01,  1.88813716e-14, -8.10871981e-01,\n",
      "       -2.59686712e-14,  7.43614897e-01, -8.96799270e-15, -6.59746685e-01])}\n"
     ]
    }
   ],
   "source": [
    "print(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6f7f7b-1beb-474a-b992-faa85a2a3400",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".llmp-venv",
   "language": "python",
   "name": ".llmp-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
