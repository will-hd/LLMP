{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbcdd28-f8ca-4995-9839-1f066b9767e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "sys.path.append('..')\n",
    "import pickle\n",
    "from typing import NamedTuple, Literal\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca990556-8a3d-4554-881e-8c8b0b3d7e1f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'NamedTuple' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mLLMPRegressionDescription\u001b[39;00m(\u001b[43mNamedTuple\u001b[49m):\n\u001b[1;32m      2\u001b[0m     x_context: torch\u001b[38;5;241m.\u001b[39mTensor\n\u001b[1;32m      3\u001b[0m     y_context: torch\u001b[38;5;241m.\u001b[39mTensor\n",
      "\u001b[0;31mNameError\u001b[0m: name 'NamedTuple' is not defined"
     ]
    }
   ],
   "source": [
    "class LLMPRegressionDescription(NamedTuple):\n",
    "    x_context: torch.Tensor\n",
    "    y_context: torch.Tensor\n",
    "    x_target: torch.Tensor\n",
    "    y_target: torch.Tensor\n",
    "    knowledge: list[str] | tuple[str] | None\n",
    "    num_total_points: int\n",
    "    num_context_points: int\n",
    "\n",
    "class TempData:\n",
    "    def __init__(self, data: pd.DataFrame, max_num_context: int, device, random_state=42):\n",
    "        self.data = data\n",
    "        self.max_num_context = max_num_context\n",
    "        print(data)\n",
    "        # x_values = np.linspace(0, 2355, 288)\n",
    "        # x_values = data.iloc[0][5:].values.astype('float32') # (288,)\n",
    "        x_values = torch.from_numpy(data.columns.values[3:].astype(int)).unsqueeze(0)\n",
    "        print(f\"x_values= {x_values}\")\n",
    "        # assert x_values.shape[0] == 288\n",
    "        self.x_values = x_values #torch.linspace(-2, 2, len(x_values), device=device).unsqueeze(0)\n",
    "        \n",
    "        y_values = data.iloc[1:, 3:].values\n",
    "        y_desc = data.iloc[1:, 2].values\n",
    "        y_values_train, y_values_temp, y_desc_train, y_desc_temp = train_test_split(\n",
    "            y_values, y_desc, test_size=0.3, random_state=random_state\n",
    "        )\n",
    "        y_values_val, y_values_test, y_desc_val, y_desc_test = train_test_split(\n",
    "            y_values_temp, y_desc_temp, test_size=0.5, random_state=random_state\n",
    "        )\n",
    "\n",
    "        self.y_values_train = torch.tensor(y_values_train).float().to(device)\n",
    "        self.y_values_val = torch.tensor(y_values_val).float().to(device)\n",
    "        self.y_values_test = torch.tensor(y_values_test).float().to(device)\n",
    "        self.y_desc_train = y_desc_train\n",
    "        self.y_desc_val = y_desc_val\n",
    "        self.y_desc_test = y_desc_test\n",
    "\n",
    "    def generate_batch(self, \n",
    "                       batch_size: int,\n",
    "                       split: Literal['train', 'val', 'test'],\n",
    "                       device: torch.device = torch.device('cpu'),\n",
    "                       return_knowledge: bool = False,\n",
    "                       num_context: None | int = None\n",
    "                       ) -> LLMPRegressionDescription:\n",
    "        # num_total_points = self.x_values.size(-1)\n",
    "        num_total_points = 288\n",
    "        if num_context is None:\n",
    "            num_context = np.random.randint(low=1, high=self.max_num_context)\n",
    "        else:\n",
    "            assert isinstance(num_context, int) \n",
    "        num_target = num_total_points  # Using all points as target\n",
    "\n",
    "        if split == 'train':\n",
    "            selected_indices = np.random.choice(self.y_values_train.size(0), batch_size, replace=False)\n",
    "            selected_y_values = self.y_values_train[selected_indices]  # Shape: [batch_size, num_points]\n",
    "\n",
    "            knowledge = self.y_desc_train[selected_indices]\n",
    "        elif split == 'val':\n",
    "            selected_indices = np.random.choice(self.y_values_val.size(0), batch_size, replace=False)\n",
    "            selected_y_values = self.y_values_val[selected_indices]  # Shape: [batch_size, num_points]\n",
    "\n",
    "            knowledge = self.y_desc_val[selected_indices]\n",
    "\n",
    "        elif split == 'test':\n",
    "            selected_indices = np.random.choice(self.y_values_test.size(0), batch_size, replace=False)\n",
    "            selected_y_values = self.y_values_test[selected_indices]  # Shape: [batch_size, num_points]\n",
    "\n",
    "            knowledge = self.y_desc_test[selected_indices]\n",
    "        else:\n",
    "            raise ValueError(\"split must be one of ['train', 'val', 'test']\")\n",
    "        # Split into context and target sets\n",
    "        context_indices = np.random.choice(num_total_points // 2, num_context, replace=False)\n",
    "\n",
    "        x_context = self.x_values[:, context_indices].repeat(batch_size, 1)  # Shape: [batch_size, num_context]\n",
    "        y_context = selected_y_values[:, context_indices]  # Shape: [batch_size, num_context]\n",
    "\n",
    "        x_target = self.x_values.repeat(batch_size, 1)  # Shape: [batch_size, num_target]\n",
    "        y_target = selected_y_values  # Shape: [batch_size, num_target]\n",
    "        \n",
    "        if return_knowledge:\n",
    "            \n",
    "            return LLMPRegressionDescription(\n",
    "                x_context=x_context.unsqueeze(-1).to(device),  # Shape: [batch_size, num_context, x_size]\n",
    "                y_context=y_context.unsqueeze(-1).to(device),  # Shape: [batch_size, num_context, y_size]\n",
    "                x_target=x_target.unsqueeze(-1).to(device),    # Shape: [batch_size, num_target, x_size]\n",
    "                y_target=y_target.unsqueeze(-1).to(device),    # Shape: [batch_size, num_target, y_size]\n",
    "                knowledge=list(knowledge), # Shape/type: TODO\n",
    "                num_total_points=num_total_points,\n",
    "                num_context_points=num_context\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            \n",
    "            return LLMPRegressionDescription(\n",
    "                x_context=x_context.unsqueeze(-1).to(device),  # Shape: [batch_size, num_context, x_size]\n",
    "                y_context=y_context.unsqueeze(-1).to(device),  # Shape: [batch_size, num_context, y_size]\n",
    "                x_target=x_target.unsqueeze(-1).to(device),    # Shape: [batch_size, num_target, x_size]\n",
    "                y_target=y_target.unsqueeze(-1).to(device),    # Shape: [batch_size, num_target, y_size]\n",
    "                knowledge=None,\n",
    "                num_total_points=num_total_points,\n",
    "                num_context_points=num_context\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4806aebc-2c68-4373-85b2-5876806cec15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_save_data(file_path, pkl_output_path, max_num_context=20, device='cpu', random_state=47, return_knowledge=False):\n",
    "\n",
    "    data = pd.read_csv(file_path)\n",
    "    \n",
    "    temp_data = TempData(data=data, max_num_context=max_num_context, device=device, random_state=random_state)\n",
    "\n",
    "    batch_size = 1\n",
    "    num_context = 10\n",
    "    test_num = 288\n",
    "    \n",
    "    llmp_description = temp_data.generate_batch(batch_size=batch_size, split='train', device=device, return_knowledge=return_knowledge, num_context=num_context)\n",
    "\n",
    "    data_to_save = {\n",
    "        'x_train': llmp_description.x_context.cpu().numpy().flatten(),\n",
    "        'y_train': llmp_description.y_context.cpu().numpy().flatten(),\n",
    "        'x_test': llmp_description.x_target.cpu().numpy().flatten()[144::6],\n",
    "        'y_test': llmp_description.y_target.cpu().numpy().flatten()[144::6],\n",
    "        'x_true': llmp_description.x_target.cpu().numpy().flatten(),\n",
    "        'y_true': llmp_description.y_target.cpu().numpy().flatten(),\n",
    "    }\n",
    "    \n",
    "    with open(pkl_output_path, 'wb') as handle:\n",
    "        pickle.dump(data_to_save, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    knowledge = None\n",
    "    if return_knowledge:\n",
    "        knowledge = llmp_description.knowledge\n",
    "    \n",
    "    return {\n",
    "        'data': data_to_save,\n",
    "        'knowledge': knowledge\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "result = process_and_save_data('../data/kasia/data_with_desc.csv', 'random_sample_output_data.pkl', return_knowledge=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfcd46f-ebf6-4a94-bd5b-fdcda8a5095a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.plot import plot_samples, plot_images, plot_heatmap\n",
    "from src.hf_api import get_model_and_tokenizer\n",
    "from src.parse_args import parse_command_line\n",
    "from src.compute_nll import compute_nll\n",
    "from src.sample import sample\n",
    "from src.prepare_data import prepare_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ab42ad-681a-49fa-9827-643a79e60ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "\n",
    "args = SimpleNamespace(\n",
    "    cfg=None,  # Assuming `ActionConfigFile` is a custom action, replace with the appropriate default if needed.\n",
    "    mode='sample_logpy',\n",
    "    experiment_name='test',\n",
    "    # data_path='../data/functions/linear_25_seed_6.pkl',\n",
    "    data_path='random_sample_output_data.pkl',\n",
    "    llm_path=None,\n",
    "    llm_type= \"llama-2-7B\", # \"llama-2-70B\",\n",
    "    prompt_ordering='distance',\n",
    "    output_dir='./output',\n",
    "    plot_dir='./plots',\n",
    "    seed=1,\n",
    "    num_decimal_places_x=0,\n",
    "    num_decimal_places_y=2,\n",
    "    batch_size=5,\n",
    "    autoregressive=True,\n",
    "    prefix= 'sinusoidally cold, then hot',\n",
    "    x_prefix='',\n",
    "    y_prefix=', ',\n",
    "    break_str='\\n',\n",
    "    sort_x_test=False,\n",
    "    forecast=True,\n",
    "    print_prompts=False,\n",
    "    print_logprobs=False,\n",
    "    num_samples=10,\n",
    "    temperature=1.0,\n",
    "    top_p=0.9,\n",
    "    max_generated_length=7,\n",
    "    y_min=None,\n",
    "    y_max=None,\n",
    "    plot_trajectories=5,\n",
    "    specify_xy=False,\n",
    "    xs=None,  # Assuming default is None when no arguments are provided\n",
    "    ys=None,  # Assuming default is None when no arguments are provided\n",
    "    xs_start=None,\n",
    "    xs_end=None,\n",
    "    num_xs=None,\n",
    "    ys_start=None,\n",
    "    ys_end=None,\n",
    "    num_ys=None,\n",
    "    mask_unused_tokens=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc9daf7d-9f3c-4c15-94e7-a283d449406d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_HOME'] = '/workspace/will/LLMP/hf_cache/' \n",
    "os.environ['HF_HUB_CACHE'] = '/workspace/will/LLMP/hf_cache/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9b20b28-3b7f-4855-a486-c612ab52cc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_map = {\n",
    "    # \"llama-2-7B\": \"meta-llama/Llama-2-7b\",  Llama-2-7b-hf\n",
    "    \"llama-2-7B\": \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    \n",
    "    \"llama-2-70B\": \"meta-llama/Llama-2-70b-hf\",\n",
    "    # \"llama-3-8B\": \"meta-llama/Meta-Llama-3-8B\", meta-llama/Meta-Llama-3.1-8B\n",
    "    \"llama-3-8B\": \"meta-llama/Meta-Llama-3.1-8B\", \n",
    "    # \"llama-3-70B\": \"meta-llama/Meta-Llama-3-70B\", meta-llama/Meta-Llama-3.1-70B\n",
    "    \"llama-3-70B\": \"meta-llama/Meta-Llama-3.1-70B\",\n",
    "    \"mixtral-8x7B\": \"mistralai/Mixtral-8x7B-v0.1\",\n",
    "    \"mixtral-8x7B-instruct\": \"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "    \"phi-3-mini-128k-instruct\": \"microsoft/Phi-3-mini-128k-instruct\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294dbb5b-f511-4916-a06a-5755ed5ef51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mistral_token = \"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\"\n",
    "# llama_token = \"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\"\n",
    "llama_chat_token = \"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\"\n",
    "# llama3_token = \"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\"\n",
    "# llama2_70b = \"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\"\n",
    "# llama3_70b = \"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\"\n",
    "# get the llm and asociated tokenizer\n",
    "model, tokenizer = get_model_and_tokenizer(args.llm_path, args.llm_type, llama_chat_token)\n",
    "# Load model directly\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B\", token=llama3_token, cache_dir=)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B\", token=llama3_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16ad9952-c8dd-4de1-a3b5-6ee0172f1e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict({'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 0, 'model.layers.9': 0, 'model.layers.10': 0, 'model.layers.11': 0, 'model.layers.12': 0, 'model.layers.13': 0, 'model.layers.14': 0, 'model.layers.15': 0, 'model.layers.16': 0, 'model.layers.17': 0, 'model.layers.18': 0, 'model.layers.19': 0, 'model.layers.20': 0, 'model.layers.21': 0, 'model.layers.22': 0, 'model.layers.23': 0, 'model.layers.24': 0, 'model.layers.25': 0, 'model.layers.26': 0, 'model.layers.27': 0, 'model.layers.28': 0, 'model.layers.29': 0, 'model.layers.30': 0, 'model.layers.31': 0, 'model.layers.32': 0, 'model.layers.33': 0, 'model.layers.34': 0, 'model.layers.35': 0, 'model.layers.36': 0, 'model.layers.37': 0, 'model.layers.38': 0, 'model.layers.39': 0, 'model.layers.40': 0, 'model.layers.41': 0, 'model.layers.42': 0, 'model.layers.43': 0, 'model.layers.44': 0, 'model.layers.45': 0, 'model.layers.46': 0, 'model.layers.47.self_attn': 0, 'model.layers.47.mlp.gate_proj': 0, 'model.layers.47.mlp.up_proj': 0, 'model.layers.47.mlp.down_proj': 1, 'model.layers.47.mlp.act_fn': 1, 'model.layers.47.input_layernorm': 1, 'model.layers.47.post_attention_layernorm': 1, 'model.layers.48': 1, 'model.layers.49': 1, 'model.layers.50': 1, 'model.layers.51': 1, 'model.layers.52': 1, 'model.layers.53': 1, 'model.layers.54': 1, 'model.layers.55': 1, 'model.layers.56': 1, 'model.layers.57': 1, 'model.layers.58': 1, 'model.layers.59': 1, 'model.layers.60': 1, 'model.layers.61': 1, 'model.layers.62': 1, 'model.layers.63': 1, 'model.layers.64': 1, 'model.layers.65': 1, 'model.layers.66': 1, 'model.layers.67': 1, 'model.layers.68': 1, 'model.layers.69': 1, 'model.layers.70': 1, 'model.layers.71': 1, 'model.layers.72': 1, 'model.layers.73': 1, 'model.layers.74': 1, 'model.layers.75': 1, 'model.layers.76': 1, 'model.layers.77': 1, 'model.layers.78': 1, 'model.layers.79': 1, 'model.norm': 1, 'model.rotary_emb': 1, 'lm_head': 1})\n"
     ]
    }
   ],
   "source": [
    "from accelerate import infer_auto_device_map\n",
    "\n",
    "device_map = infer_auto_device_map(model, max_memory={0: \"80GiB\", 1: \"80GiB\",  \"cpu\": \"250GiB\"})\n",
    "print(device_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52b244cb-5db5-46ef-9746-df9a782b452e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from src.run_llm_process import run_llm_process\\nrun_llm_process(args=args, model=model, tokenizer=tokenizer)'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from src.run_llm_process import run_llm_process\n",
    "run_llm_process(args=args, model=model, tokenizer=tokenizer)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f786cf10-3bc4-41f6-b954-381aaf7dff2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a06c6e4-8d95-4daa-85c3-5ee4084d9c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     LST_DATE  LST_DATE.1                                        description  \\\n",
      "0    20210101    20210101  The night will start off very chilly at around...   \n",
      "1    20210102    20210102  The night will start off cold with temperature...   \n",
      "2    20210103    20210103  The night will start off cold at -16.4°C and g...   \n",
      "3    20210104    20210104  The night will start bitterly cold with temper...   \n",
      "4    20210105    20210105  The night will be bitterly cold with temperatu...   \n",
      "..        ...         ...                                                ...   \n",
      "720  20221226    20221226  The night will be bitterly cold with temperatu...   \n",
      "721  20221227    20221227  The night will start off cold with temperature...   \n",
      "722  20221228    20221228  It's going to be a chilly day with temperature...   \n",
      "723  20221229    20221229  The night will start off cold with temperature...   \n",
      "724  20221230    20221230  The night will start off cold with temperature...   \n",
      "\n",
      "        0     5    10    15    20    25    30  ...  2310  2315  2320  2325  \\\n",
      "0    -5.9  -5.6  -5.2  -4.7  -4.8  -3.5  -4.3  ...  -7.1  -7.1  -7.4  -7.8   \n",
      "1    -8.4  -8.5  -8.8  -8.9  -9.1  -9.3  -9.2  ... -16.4 -16.2 -16.1 -16.2   \n",
      "2   -16.4 -16.4 -16.4 -16.3 -16.3 -16.3 -16.5  ... -18.4 -18.2 -18.1 -18.2   \n",
      "3   -18.1 -18.0 -17.8 -17.9 -17.9 -17.9 -17.9  ...  -7.7  -7.7  -7.7  -7.6   \n",
      "4    -7.9  -7.8  -7.9  -8.1  -8.1  -8.2  -8.3  ...  -7.3  -7.4  -7.6  -7.7   \n",
      "..    ...   ...   ...   ...   ...   ...   ...  ...   ...   ...   ...   ...   \n",
      "720 -14.8 -14.8 -14.8 -14.8 -14.8 -14.8 -14.8  ... -11.5 -11.6 -11.6 -11.7   \n",
      "721 -11.3 -11.3 -11.3 -11.3 -11.2 -11.3 -11.2  ... -12.5 -12.4 -12.4 -12.3   \n",
      "722 -12.2 -12.2 -12.2 -12.2 -12.2 -12.1 -12.1  ...  -2.1  -2.1  -2.1  -2.1   \n",
      "723  -2.1  -2.2  -2.2  -2.2  -2.2  -2.1  -2.0  ...  -4.0  -4.0  -4.1  -4.2   \n",
      "724  -4.4  -4.3  -4.2  -4.2  -4.3  -4.3  -4.4  ...  -2.4  -2.4  -2.4  -2.4   \n",
      "\n",
      "     2330  2335  2340  2345  2350  2355  \n",
      "0    -7.8  -8.0  -7.7  -8.1  -7.9  -8.0  \n",
      "1   -16.0 -16.3 -16.1 -16.3 -16.3 -16.3  \n",
      "2   -18.1 -18.2 -18.2 -18.3 -18.3 -18.2  \n",
      "3    -7.7  -7.8  -7.8  -7.9  -7.9  -7.9  \n",
      "4    -7.7  -7.6  -7.5  -7.4  -7.3  -7.3  \n",
      "..    ...   ...   ...   ...   ...   ...  \n",
      "720 -11.5 -11.4 -11.3 -11.3 -11.2 -11.3  \n",
      "721 -12.3 -12.3 -12.2 -12.2 -12.2 -12.2  \n",
      "722  -2.0  -2.0  -2.0  -2.0  -2.0  -2.1  \n",
      "723  -4.3  -4.3  -4.3  -4.4  -4.5  -4.5  \n",
      "724  -2.4  -2.3  -2.3  -2.4  -2.4  -2.4  \n",
      "\n",
      "[725 rows x 291 columns]\n",
      "x_values= tensor([[   0,    5,   10,   15,   20,   25,   30,   35,   40,   45,   50,   55,\n",
      "          100,  105,  110,  115,  120,  125,  130,  135,  140,  145,  150,  155,\n",
      "          200,  205,  210,  215,  220,  225,  230,  235,  240,  245,  250,  255,\n",
      "          300,  305,  310,  315,  320,  325,  330,  335,  340,  345,  350,  355,\n",
      "          400,  405,  410,  415,  420,  425,  430,  435,  440,  445,  450,  455,\n",
      "          500,  505,  510,  515,  520,  525,  530,  535,  540,  545,  550,  555,\n",
      "          600,  605,  610,  615,  620,  625,  630,  635,  640,  645,  650,  655,\n",
      "          700,  705,  710,  715,  720,  725,  730,  735,  740,  745,  750,  755,\n",
      "          800,  805,  810,  815,  820,  825,  830,  835,  840,  845,  850,  855,\n",
      "          900,  905,  910,  915,  920,  925,  930,  935,  940,  945,  950,  955,\n",
      "         1000, 1005, 1010, 1015, 1020, 1025, 1030, 1035, 1040, 1045, 1050, 1055,\n",
      "         1100, 1105, 1110, 1115, 1120, 1125, 1130, 1135, 1140, 1145, 1150, 1155,\n",
      "         1200, 1205, 1210, 1215, 1220, 1225, 1230, 1235, 1240, 1245, 1250, 1255,\n",
      "         1300, 1305, 1310, 1315, 1320, 1325, 1330, 1335, 1340, 1345, 1350, 1355,\n",
      "         1400, 1405, 1410, 1415, 1420, 1425, 1430, 1435, 1440, 1445, 1450, 1455,\n",
      "         1500, 1505, 1510, 1515, 1520, 1525, 1530, 1535, 1540, 1545, 1550, 1555,\n",
      "         1600, 1605, 1610, 1615, 1620, 1625, 1630, 1635, 1640, 1645, 1650, 1655,\n",
      "         1700, 1705, 1710, 1715, 1720, 1725, 1730, 1735, 1740, 1745, 1750, 1755,\n",
      "         1800, 1805, 1810, 1815, 1820, 1825, 1830, 1835, 1840, 1845, 1850, 1855,\n",
      "         1900, 1905, 1910, 1915, 1920, 1925, 1930, 1935, 1940, 1945, 1950, 1955,\n",
      "         2000, 2005, 2010, 2015, 2020, 2025, 2030, 2035, 2040, 2045, 2050, 2055,\n",
      "         2100, 2105, 2110, 2115, 2120, 2125, 2130, 2135, 2140, 2145, 2150, 2155,\n",
      "         2200, 2205, 2210, 2215, 2220, 2225, 2230, 2235, 2240, 2245, 2250, 2255,\n",
      "         2300, 2305, 2310, 2315, 2320, 2325, 2330, 2335, 2340, 2345, 2350, 2355]])\n",
      "Processing row 388:\n",
      "['The night will start bitterly cold with temperatures falling to -30.2°C by sunrise, and despite a slight rise around midday, the afternoon will continue in the deep freeze, dropping back to -31.6°C by midnight. Please bundle up and stay warm throughout the day.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 2it [11:13, 336.84s/it]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 11.59 GiB. GPU 1 has a total capacity of 93.00 GiB of which 11.46 GiB is free. Process 3373751 has 81.54 GiB memory in use. Of the allocated memory 75.61 GiB is allocated by PyTorch, and 5.27 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 69\u001b[0m\n\u001b[1;32m     66\u001b[0m         plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# Example call to the function\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m \u001b[43mrun_training_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../data/kasia/data_with_desc.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./loop_output_dir\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 65\u001b[0m, in \u001b[0;36mrun_training_loop\u001b[0;34m(data_file, output_dir, model, tokenizer, total_number)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mknowledge\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     23\u001b[0m args \u001b[38;5;241m=\u001b[39m SimpleNamespace(\n\u001b[1;32m     24\u001b[0m     cfg\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     25\u001b[0m     mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample_logpy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     62\u001b[0m     mask_unused_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     63\u001b[0m )\n\u001b[0;32m---> 65\u001b[0m \u001b[43mrun_llm_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m/workspace/will/LLMP/my_notebooks/../src/run_llm_process.py:22\u001b[0m, in \u001b[0;36mrun_llm_process\u001b[0;34m(args, model, tokenizer)\u001b[0m\n\u001b[1;32m     19\u001b[0m results \u001b[38;5;241m=\u001b[39m prepare_data(args)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m args\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m---> 22\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresults\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogpy\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m args\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m     25\u001b[0m     results \u001b[38;5;241m=\u001b[39m compute_nll(args, tokenizer, model, results)\n",
      "File \u001b[0;32m/workspace/will/LLMP/my_notebooks/../src/sample.py:47\u001b[0m, in \u001b[0;36msample\u001b[0;34m(args, tokenizer, model, results)\u001b[0m\n\u001b[1;32m     44\u001b[0m     batch_prompts\u001b[38;5;241m.\u001b[39mappend(prompt[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;66;03m# print(prompt)\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# print(\"new sample\")\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mhf_generate_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_prompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_generated_length\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m(\u001b[38;5;28mlen\u001b[39m(res) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_indices))\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, sample_index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(batch_indices):\n",
      "File \u001b[0;32m/workspace/will/LLMP/.llmp-venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/will/LLMP/my_notebooks/../src/hf_api.py:174\u001b[0m, in \u001b[0;36mhf_generate_batch\u001b[0;34m(model, tokenizer, prompts, temp, top_p, max_new_tokens)\u001b[0m\n\u001b[1;32m    171\u001b[0m batch \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mcuda() \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m    172\u001b[0m num_input_ids \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m--> 174\u001b[0m generate_ids \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrenormalize_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;66;03m# print(f\"generate_ids shape.: {generate_ids.shape}\")\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;66;03m# print(f\"generate_ids[0]: {generate_ids[0, num_input_ids:]}\")\u001b[39;00m\n\u001b[1;32m    185\u001b[0m gen_strs \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(\n\u001b[1;32m    186\u001b[0m     generate_ids[:, num_input_ids:],\n\u001b[1;32m    187\u001b[0m     skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    188\u001b[0m     clean_up_tokenization_spaces\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    189\u001b[0m )\n",
      "File \u001b[0;32m/workspace/will/LLMP/.llmp-venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/will/LLMP/.llmp-venv/lib/python3.12/site-packages/transformers/generation/utils.py:2024\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2016\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2017\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2018\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2019\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2020\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2021\u001b[0m     )\n\u001b[1;32m   2023\u001b[0m     \u001b[38;5;66;03m# 13. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2024\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2025\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2026\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2027\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2028\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2029\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2030\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2031\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2032\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2033\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2035\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2036\u001b[0m     \u001b[38;5;66;03m# 11. prepare logits warper\u001b[39;00m\n\u001b[1;32m   2037\u001b[0m     prepared_logits_warper \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2038\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_logits_warper(generation_config, device\u001b[38;5;241m=\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   2039\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m generation_config\u001b[38;5;241m.\u001b[39mdo_sample\n\u001b[1;32m   2040\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2041\u001b[0m     )\n",
      "File \u001b[0;32m/workspace/will/LLMP/.llmp-venv/lib/python3.12/site-packages/transformers/generation/utils.py:2982\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, logits_warper, **model_kwargs)\u001b[0m\n\u001b[1;32m   2979\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[1;32m   2981\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2982\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   2984\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2985\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/will/LLMP/.llmp-venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/will/LLMP/.llmp-venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/will/LLMP/.llmp-venv/lib/python3.12/site-packages/accelerate/hooks.py:169\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/workspace/will/LLMP/.llmp-venv/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:1209\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1207\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1208\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n\u001b[0;32m-> 1209\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mlogits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1211\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1212\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1213\u001b[0m     \u001b[38;5;66;03m# Shift so that tokens < n predict n\u001b[39;00m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 11.59 GiB. GPU 1 has a total capacity of 93.00 GiB of which 11.46 GiB is free. Process 3373751 has 81.54 GiB memory in use. Of the allocated memory 75.61 GiB is allocated by PyTorch, and 5.27 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from src.run_llm_process import run_llm_process\n",
    "\n",
    "plt.style.use('./thesis.mplstyle')\n",
    "\n",
    "def run_training_loop(data_file, output_dir, model, tokenizer, total_number=4):\n",
    "    input_data = pd.read_csv(data_file)\n",
    "\n",
    "    # Shuffle the indices and select a subset of them\n",
    "    indices = list(input_data.index)\n",
    "    random.shuffle(indices)\n",
    "    selected_indices = indices[:total_number]\n",
    "\n",
    "    for index in selected_indices:\n",
    "        row = input_data.loc[index]\n",
    "        pkl_output_path = os.path.join(output_dir, f'sample_{index}.pkl')\n",
    "        \n",
    "        # Process the data row and save it\n",
    "        result = process_and_save_data(data_file, pkl_output_path, return_knowledge=True, random_state=random.randint(0, 10000))\n",
    "        print(f\"Processing row {index}:\")\n",
    "        print(result['knowledge'])\n",
    "\n",
    "        args = SimpleNamespace(\n",
    "            cfg=None,\n",
    "            mode='sample_logpy',\n",
    "            experiment_name='test',\n",
    "            data_path=pkl_output_path,\n",
    "            llm_path=None,\n",
    "            llm_type=\"llama2_70b\",\n",
    "            prompt_ordering='distance',\n",
    "            output_dir='./output',\n",
    "            plot_dir='./plots',\n",
    "            seed=1,\n",
    "            num_decimal_places_x=0,\n",
    "            num_decimal_places_y=1,\n",
    "            batch_size=1,\n",
    "            autoregressive=True,\n",
    "            prefix= result['knowledge'], # change to knowledge # Predict the temperature throughout the day (mins, temp): \n",
    "            x_prefix='',\n",
    "            y_prefix=', ',\n",
    "            break_str='\\n',\n",
    "            sort_x_test=False,\n",
    "            forecast=True,\n",
    "            print_prompts=False,\n",
    "            print_logprobs=False,\n",
    "            num_samples=20,\n",
    "            temperature=1.0,\n",
    "            top_p=0.9,\n",
    "            max_generated_length=5,\n",
    "            y_min=None,\n",
    "            y_max=None,\n",
    "            plot_trajectories=0,\n",
    "            specify_xy=False,\n",
    "            xs=None,\n",
    "            ys=None,\n",
    "            xs_start=None,\n",
    "            xs_end=None,\n",
    "            num_xs=None,\n",
    "            ys_start=None,\n",
    "            ys_end=None,\n",
    "            num_ys=None,\n",
    "            mask_unused_tokens=True\n",
    "        )\n",
    "\n",
    "        run_llm_process(args=args, model=model, tokenizer=tokenizer)\n",
    "        plt.show()\n",
    "\n",
    "# Example call to the function\n",
    "run_training_loop('../data/kasia/data_with_desc.csv', './loop_output_dir', model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494c4e35-1781-4e04-a36d-ace22bb861b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81863d51-e883-4a43-a371-a653bee7d9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "cwd = os.getcwd()\n",
    "cwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a814547b-7c7c-4420-9554-09786fda4121",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/functions/beat_05_seed_9.pkl','rb') as f:\n",
    "     contents = pickle.load(f)\n",
    "# '../data/weather/weather_llm_proc_10.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf31d958-982d-4e44-979f-f514ea6ebc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6f7f7b-1beb-474a-b992-faa85a2a3400",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".llmp-venv",
   "language": "python",
   "name": ".llmp-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
